# -*- coding: utf-8 -*-
"""Image reconstruction with Autoencoder.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16FcRGmYQBWWkaLVMlmAgVdp4JpiU0hEM
"""

from google.colab import drive 
drive.mount('/content/drive')

import time
from matplotlib.pyplot import imshow
import numpy as np
import cv2
from PIL import Image
import matplotlib.pyplot as plt
from keras.preprocessing.image import img_to_array
from tensorflow.keras.layers import Conv2D, MaxPooling2D, UpSampling2D
from tensorflow.keras.models import Sequential
import random
import tensorflow as tf
random.seed(2)
tf.random.set_seed(4)


start = time.time()
SIZE= 400  #Limiting to 480 size image as my laptop cannot handle larger images. 
img_data=[]

path = '/content/drive/My Drive/Untitled folder/HansRosling.jpg'

img=cv2.imread(path, 1)   #Change 1 to 0 for Grey scale images
img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  #Changing BGR to RGB to show images in true colors
img=cv2.resize(img,(SIZE, SIZE))
img_data.append(img_to_array(img))

img_array = np.reshape(img_data, (len(img_data), SIZE, SIZE, 3))
img_array = img_array.astype('float32') / 399.
print(img_array)

plt.imshow(img)

#Define Autoencoder model. 
#Encoder
model = Sequential()
model.add(Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(SIZE, SIZE, 3)))
model.add(MaxPooling2D((2, 2), padding='same'))
model.add(Conv2D(8, (3, 3), activation='relu', padding='same'))
model.add(MaxPooling2D((1, 1), padding='same'))
 

model.add(MaxPooling2D((2, 2), padding='same'))
 #Decoder   
model.add(UpSampling2D((1, 1)))
model.add(Conv2D(8, (3, 3), activation='relu', padding='same'))
model.add(UpSampling2D((2, 2)))
model.add(Conv2D(32, (3, 3), activation='relu', padding='same'))
model.add(UpSampling2D((2, 2)))
model.add(Conv2D(3, (3, 3), activation='relu', padding='same'))

model.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])
model.summary()

model.fit(img_array, img_array,
        epochs=500,                  #1000s of epochs needed for good results. Use GPU.
        shuffle=True)           #Shuffle data for each epoch

print("Output")


pred = model.predict(img_array)   #Predict model on the same input array.

#In reality, train on 1000s of input images and predict on images that the training 
#algorithm never saw. 

imshow(pred[0].reshape(SIZE,SIZE,3), cmap="gray")

img2 = Image.fromarray(pred[0], 'RGB')
pred = model.predict(img_array)   #Predict model on the same input array.

def mse(imageA, imageB):
	# the 'Mean Squared Error' between the two images is the
	# sum of the squared difference between the two images;
	# NOTE: the two images must have the same dimension
	err = np.sum((imageA.astype("float") - imageB.astype("float")) ** 2)
	err /= float(imageA.shape[0] * imageA.shape[1])
	
	# return the MSE, the lower the error, the more "similar"
	# the two images are
	return err

print(mse(pred, img_array))

end = time.time()
time_n = end - start
print(time_n)

import numpy
import tensorflow.keras.layers
import tensorflow.keras.models
import tensorflow.keras.optimizers  
import numpy
import matplotlib.pyplot

start = time.time()
im=cv2.imread(path, 1)   #Change 1 to 0 for Grey scale images
im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)  #Changing BGR to RGB to show images in true colors
im=cv2.resize(im,(SIZE, SIZE))
img_array = numpy.array(im)/149
print(img_array)
img_array.shape
plt.imshow(img_array)


# compress the matrix of a single channel
def compressSingleChannel(channelDataMatrix, singularValuesLimit):
    uChannel, sChannel, vhChannel = numpy.linalg.svd(channelDataMatrix)
    aChannelCompressed = numpy.zeros((channelDataMatrix.shape[0], channelDataMatrix.shape[1]))
    k = singularValuesLimit

    leftSide = numpy.matmul(uChannel[:, 0:k], numpy.diag(sChannel)[0:k, 0:k])
    aChannelCompressedInner = numpy.matmul(leftSide, vhChannel[0:k, :])
    aChannelCompressed = aChannelCompressedInner.astype('uint8')
    return aChannelCompressed

SIZE= 400  #Limiting to 480 size image as my laptop cannot handle larger images. 
path = '/content/drive/My Drive/Untitled folder/HansRosling.jpg'

a = cv2.imread(path, 1)   #Change 1 to 0 for Grey scale images
img = cv2.cvtColor(a, cv2.COLOR_BGR2RGB)
a=cv2.resize(a,(SIZE, SIZE))
a_np = np.array(a)/149
aRed = a_np[:,:,0]
aGreen = a_np[:,:,1]
aBlue = a_np[:,:,2]

# image width and height:
imageWidth = 150
imageHeight = 150

# number of singular values to use for reconstructing the compressed image
singularValuesLimit = 12


aRedCompressed = compressSingleChannel(aRed, singularValuesLimit)
aGreenCompressed = compressSingleChannel(aGreen, singularValuesLimit)
aBlueCompressed = compressSingleChannel(aBlue, singularValuesLimit)

imr = Image.fromarray(aRedCompressed, mode=None)
img = Image.fromarray(aGreenCompressed, mode=None)
imb = Image.fromarray(aBlueCompressed, mode=None)

newImage = Image.merge("RGB", (imr, img, imb))
P = numpy.asarray(newImage)
print(mse(P,a_np))
end = time.time()
time_n = end - start
print(time_n)

###### This part is the same content as the Image reconstruction with Convolutional Autoencoder.ipynb file

import tensorflow.keras.layers
import tensorflow.keras.models

start = time.time()
#Encoder
x = tensorflow.keras.layers.Input(shape=(67500,), name="encoder_input")

encoder_dense_layer1 = tensorflow.keras.layers.Dense(units=1100, name="encoder_dense_1")(x)
encoder_activ_layer1 = tensorflow.keras.layers.LeakyReLU(name="encoder_leakyrelu_1")(encoder_dense_layer1)

encoder_dense_layer2 = tensorflow.keras.layers.Dense(units=432, name="encoder_dense_2")(encoder_activ_layer1)
encoder_output = tensorflow.keras.layers.LeakyReLU(name="encoder_output")(encoder_dense_layer2)
encoder = tensorflow.keras.models.Model(x, encoder_output, name="encoder_model")
encoder.summary()

#Decoder
decoder_input = tensorflow.keras.layers.Input(shape=(432,), name="decoder_input")

decoder_dense_layer1 = tensorflow.keras.layers.Dense(units=1100, name="decoder_dense_1")(decoder_input)
decoder_activ_layer1 = tensorflow.keras.layers.LeakyReLU(name="decoder_leakyrelu_1")(decoder_dense_layer1)

decoder_dense_layer2 = tensorflow.keras.layers.Dense(units=67500, name="decoder_dense_2")(decoder_activ_layer1)
decoder_output = tensorflow.keras.layers.LeakyReLU(name="decoder_output")(decoder_dense_layer2)
decoder = tensorflow.keras.models.Model(decoder_input, decoder_output, name="decoder_model")
decoder.summary()

SIZE= 150  #Limiting to 480 size image as my laptop cannot handle larger images. 
img_data=[]

path = '/content/drive/My Drive/Untitled folder/HansRosling.jpg'

img=cv2.imread(path, 1)   #Change 1 to 0 for Grey scale images
img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  #Changing BGR to RGB to show images in true colors
img=cv2.resize(img,(SIZE, SIZE))


img_data.append(img_to_array(img))

img_array = np.reshape(img_data, (len(img_data), SIZE, SIZE, 3))
img_array = img_array.astype('float32') / 149.
print(img_array.shape)

x_train = img_array.flatten()
x_train = x_train.reshape(1,67500)
x_train.shape

ae_input = tensorflow.keras.layers.Input(shape=(67500), name="AE_input")
ae_encoder_output = encoder(ae_input)
ae_decoder_output = decoder(ae_encoder_output)

ae = tensorflow.keras.models.Model(ae_input, ae_decoder_output, name="AE")
import tensorflow.keras.optimizers  
ae.compile(loss="mse", optimizer=tensorflow.keras.optimizers.Adam(lr=0.0005))
ae.fit(x_train, x_train, epochs=100, batch_size=256, shuffle=True)

encoded_images = encoder.predict(x_train)
decoded_images = decoder.predict(encoded_images)
y = decoded_images.reshape(150,150,3)
print(mse(y ,img_array))
end = time.time()
time_n = end - start
print(time_n)

img=cv2.imread(path, 1)   #Change 1 to 0 for Grey scale images
img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  #Changing BGR to RGB to show images in true colors
img=cv2.resize(img,(SIZE, SIZE))/149
plt.imshow(img)

u = encoded_images.reshape(12,12,3)
u.shape
plt.imshow(u)

y = decoded_images.reshape(150,150,3)
print(y.shape)
plt.imshow(y)